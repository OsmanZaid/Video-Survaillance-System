{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12b8c6b5",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "793e447c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import imutils\n",
    "from keras.preprocessing.image import img_to_array,load_img\n",
    "from keras.layers import Conv3D,ConvLSTM2D,Conv3DTranspose,Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af0b2eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win10\n"
     ]
    }
   ],
   "source": [
    "os.chdir('C:\\\\Users\\\\win10')\n",
    "owd = os.getcwd()\n",
    "print(owd)\n",
    "os.chdir(owd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded1946d",
   "metadata": {},
   "source": [
    "## Setting directory path variable and Function to process and store video frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d22b473a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['01.mp4', '02.mp4', '03.mp4', '04.mp4', '05.mp4', '06.mp4', '07.mp4', '08.mp4', '09.mp4', '10.mp4', '11.mp4', '12.mp4', '13.mp4', '14.mp4', '15.mp4', '16.mp4']\n"
     ]
    }
   ],
   "source": [
    "store_image=[]\n",
    "train_path='C:\\\\Projects\\\\Video Surveilance\\\\Train path'\n",
    "fps=5\n",
    "train_videos=os.listdir('C:\\\\Projects\\\\Video Surveilance\\\\Avenue_Dataset\\\\Avenue Dataset\\\\training_videos')\n",
    "print(train_videos)\n",
    "train_images_path=train_path+'/frames'\n",
    "# os.makedirs(train_images_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fbc497",
   "metadata": {},
   "source": [
    "## Extract frames from video and call store function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0947eb94",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<VideoCapture 0000026D75102C10>, <VideoCapture 0000026D69EA9CD0>, <VideoCapture 0000026D69EA9C30>, <VideoCapture 0000026D751D5210>, <VideoCapture 0000026D751D5050>, <VideoCapture 0000026D69EA9BB0>, <VideoCapture 0000026D751D51F0>, <VideoCapture 0000026D751D50D0>, <VideoCapture 0000026D751D53B0>, <VideoCapture 0000026D751D5230>, <VideoCapture 0000026D751D53D0>, <VideoCapture 0000026D751D52B0>, <VideoCapture 0000026D751D52D0>, <VideoCapture 0000026D751D5070>, <VideoCapture 0000026D751D51B0>, <VideoCapture 0000026D751D5290>]\n",
      "Video - 1365\n",
      "Video - 1512\n",
      "Video - 1488\n",
      "Video - 1512\n",
      "Video - 816\n",
      "Video - 1512\n",
      "Video - 1100\n",
      "Video - 1018\n",
      "Video - 1392\n",
      "Video - 1224\n",
      "Video - 782\n",
      "Video - 146\n",
      "Video - 367\n",
      "Video - 511\n",
      "Video - 354\n",
      "Video - 245\n"
     ]
    }
   ],
   "source": [
    "# print(train_videos)\n",
    "os.chdir(train_images_path)\n",
    "# for video in train_videos:\n",
    "#     print(video)\n",
    "vid = [cv2.VideoCapture(f'C:\\\\Projects\\\\Video Surveilance\\\\Avenue_Dataset\\\\Avenue Dataset\\\\training_videos\\\\{path}') for path in train_videos]\n",
    "print(vid)\n",
    "v=1\n",
    "for vidcap in vid:\n",
    "    success,image = vidcap.read()\n",
    "    count=1;\n",
    "    while success:\n",
    "        cv2.imwrite(\"Vid\"+str(v)+\"-%d.jpg\" % count, image)\n",
    "        success,image = vidcap.read()\n",
    "        count += 1\n",
    "    print(\"Video -\",count)\n",
    "    v+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe83395",
   "metadata": {},
   "source": [
    "## Store the store_image list in a numpy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df004133",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Images Processed\n",
      "200 Images Processed\n",
      "300 Images Processed\n",
      "400 Images Processed\n",
      "500 Images Processed\n",
      "600 Images Processed\n",
      "700 Images Processed\n",
      "800 Images Processed\n",
      "900 Images Processed\n",
      "1000 Images Processed\n",
      "1100 Images Processed\n",
      "1200 Images Processed\n",
      "1300 Images Processed\n",
      "1400 Images Processed\n",
      "1500 Images Processed\n",
      "1600 Images Processed\n",
      "1700 Images Processed\n",
      "1800 Images Processed\n",
      "1900 Images Processed\n",
      "2000 Images Processed\n",
      "2100 Images Processed\n",
      "2200 Images Processed\n",
      "2300 Images Processed\n",
      "2400 Images Processed\n",
      "2500 Images Processed\n",
      "2600 Images Processed\n",
      "2700 Images Processed\n",
      "2800 Images Processed\n",
      "2900 Images Processed\n",
      "3000 Images Processed\n",
      "3100 Images Processed\n",
      "3200 Images Processed\n",
      "3300 Images Processed\n",
      "3400 Images Processed\n",
      "3500 Images Processed\n",
      "3600 Images Processed\n",
      "3700 Images Processed\n",
      "3800 Images Processed\n",
      "3900 Images Processed\n",
      "4000 Images Processed\n",
      "4100 Images Processed\n",
      "4200 Images Processed\n",
      "4300 Images Processed\n",
      "4400 Images Processed\n",
      "4500 Images Processed\n",
      "4600 Images Processed\n",
      "4700 Images Processed\n",
      "4800 Images Processed\n",
      "4900 Images Processed\n",
      "5000 Images Processed\n",
      "5100 Images Processed\n",
      "5200 Images Processed\n",
      "5300 Images Processed\n",
      "5400 Images Processed\n",
      "5500 Images Processed\n",
      "5600 Images Processed\n",
      "5700 Images Processed\n",
      "5800 Images Processed\n",
      "5900 Images Processed\n",
      "6000 Images Processed\n",
      "All Images Processed\n",
      "6000 227 227\n"
     ]
    }
   ],
   "source": [
    "os.chdir(train_path)\n",
    "store_image = []\n",
    "\n",
    "def store_inarray(image_path):\n",
    "    image=load_img(image_path)\n",
    "    image=img_to_array(image)\n",
    "    image=cv2.resize(image, (227,227), interpolation = cv2.INTER_AREA)\n",
    "    gray=0.2989*image[:,:,0]+0.5870*image[:,:,1]+0.1140*image[:,:,2]\n",
    "    store_image.append(gray)\n",
    "\n",
    "images=os.listdir(train_images_path)\n",
    "i=0\n",
    "for image in images:\n",
    "    i+=1\n",
    "    if i<=6000:\n",
    "        if(i%100==0):\n",
    "            print(i,\"Images Processed\")\n",
    "        \n",
    "        image_path=train_images_path + '\\\\' + image\n",
    "        store_inarray(image_path)    \n",
    "\n",
    "print(\"All Images Processed\")\n",
    "store_image=np.array(store_image)\n",
    "a,b,c = store_image.shape\n",
    "print(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0454655",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_image.resize(b,c,a)\n",
    "store_image=(store_image-store_image.mean())/(store_image.std())\n",
    "store_image=np.clip(store_image,0,1)\n",
    "np.save('training.npy',store_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa8826cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stae_model=Sequential()\n",
    "stae_model.add(Conv3D(filters=128,kernel_size=(11,11,1),strides=(4,4,1),padding='valid',input_shape=(227,227,10,1),activation='tanh'))\n",
    "stae_model.add(Conv3D(filters=64,kernel_size=(5,5,1),strides=(2,2,1),padding='valid',activation='tanh'))\n",
    "stae_model.add(ConvLSTM2D(filters=64,kernel_size=(3,3),strides=1,padding='same',dropout=0.4,recurrent_dropout=0.3,return_sequences=True))\n",
    "stae_model.add(ConvLSTM2D(filters=32,kernel_size=(3,3),strides=1,padding='same',dropout=0.3,return_sequences=True))\n",
    "stae_model.add(ConvLSTM2D(filters=64,kernel_size=(3,3),strides=1,return_sequences=True, padding='same',dropout=0.5))\n",
    "stae_model.add(Conv3DTranspose(filters=128,kernel_size=(5,5,1),strides=(2,2,1),padding='valid',activation='tanh'))\n",
    "stae_model.add(Conv3DTranspose(filters=1,kernel_size=(11,11,1),strides=(4,4,1),padding='valid',activation='tanh'))\n",
    "stae_model.compile(optimizer='adam',loss='mean_squared_error',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01c6d0f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d (Conv3D)             (None, 55, 55, 10, 128)   15616     \n",
      "                                                                 \n",
      " conv3d_1 (Conv3D)           (None, 26, 26, 10, 64)    204864    \n",
      "                                                                 \n",
      " conv_lstm2d (ConvLSTM2D)    (None, 26, 26, 10, 64)    295168    \n",
      "                                                                 \n",
      " conv_lstm2d_1 (ConvLSTM2D)  (None, 26, 26, 10, 32)    110720    \n",
      "                                                                 \n",
      " conv_lstm2d_2 (ConvLSTM2D)  (None, 26, 26, 10, 64)    221440    \n",
      "                                                                 \n",
      " conv3d_transpose (Conv3DTra  (None, 55, 55, 10, 128)  204928    \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv3d_transpose_1 (Conv3DT  (None, 227, 227, 10, 1)  15489     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,068,225\n",
      "Trainable params: 1,068,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "stae_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d6752ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "600/600 [==============================] - ETA: 0s - loss: 0.0864 - accuracy: 0.7092WARNING:tensorflow:Can save best model only with mean_squared_error available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "600/600 [==============================] - 514s 836ms/step - loss: 0.0864 - accuracy: 0.7092\n",
      "Epoch 2/3\n",
      "600/600 [==============================] - ETA: 0s - loss: 0.0487 - accuracy: 0.7521WARNING:tensorflow:Can save best model only with mean_squared_error available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "600/600 [==============================] - 499s 832ms/step - loss: 0.0487 - accuracy: 0.7521\n",
      "Epoch 3/3\n",
      "600/600 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.7738WARNING:tensorflow:Can save best model only with mean_squared_error available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "600/600 [==============================] - 503s 839ms/step - loss: 0.0289 - accuracy: 0.7738\n"
     ]
    }
   ],
   "source": [
    "training_data=np.load('training.npy')\n",
    "frames=training_data.shape[2]\n",
    "frames=frames-frames%10\n",
    "training_data=training_data[:,:,:frames]\n",
    "training_data=training_data.reshape(-1,227,227,10)\n",
    "training_data=np.expand_dims(training_data,axis=4)\n",
    "target_data=training_data.copy()\n",
    "epochs=3\n",
    "batch_size=1\n",
    "callback_save = ModelCheckpoint(\"saved_model.h5\", monitor=\"mean_squared_error\", save_best_only=True)\n",
    "callback_early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "stae_model.fit(training_data,target_data, batch_size=batch_size, epochs=epochs, callbacks = [callback_save,callback_early_stopping])\n",
    "stae_model.save(\"saved_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a6dfe5",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b738c9ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'KerasLazyLoader' from 'tensorflow.python.util.lazy_loader' (e:\\Personal Projects\\Video Surveilence System\\venv\\lib\\site-packages\\tensorflow\\python\\util\\lazy_loader.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32me:\\Personal Projects\\Video Surveilence System\\Video Surveilance.ipynb Cell 15\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Personal%20Projects/Video%20Surveilence%20System/Video%20Surveilance.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Personal%20Projects/Video%20Surveilence%20System/Video%20Surveilance.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mimutils\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Personal%20Projects/Video%20Surveilence%20System/Video%20Surveilance.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m load_model\n",
      "File \u001b[1;32me:\\Personal Projects\\Video Surveilence System\\venv\\lib\\site-packages\\keras\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m __internal__\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m activations\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m applications\n",
      "File \u001b[1;32me:\\Personal Projects\\Video Surveilence System\\venv\\lib\\site-packages\\keras\\__internal__\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m__internal__\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m__internal__\u001b[39;00m \u001b[39mimport\u001b[39;00m layers\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m__internal__\u001b[39;00m \u001b[39mimport\u001b[39;00m losses\n",
      "File \u001b[1;32me:\\Personal Projects\\Video Surveilence System\\venv\\lib\\site-packages\\keras\\__internal__\\backend\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m \u001b[39mimport\u001b[39;00m _initialize_variables \u001b[39mas\u001b[39;00m initialize_variables\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m \u001b[39mimport\u001b[39;00m track_variable\n",
      "File \u001b[1;32me:\\Personal Projects\\Video Surveilence System\\venv\\lib\\site-packages\\keras\\src\\__init__.py:21\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m applications\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n",
      "File \u001b[1;32me:\\Personal Projects\\Video Surveilence System\\venv\\lib\\site-packages\\keras\\src\\applications\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Keras Applications are premade architectures with pre-trained weights.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapplications\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconvnext\u001b[39;00m \u001b[39mimport\u001b[39;00m ConvNeXtBase\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapplications\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconvnext\u001b[39;00m \u001b[39mimport\u001b[39;00m ConvNeXtLarge\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapplications\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconvnext\u001b[39;00m \u001b[39mimport\u001b[39;00m ConvNeXtSmall\n",
      "File \u001b[1;32me:\\Personal Projects\\Video Surveilence System\\venv\\lib\\site-packages\\keras\\src\\applications\\convnext.py:26\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39m\"\"\"ConvNeXt models for Keras.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[39mReferences:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39m  (CVPR 2022)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m initializers\n",
      "File \u001b[1;32me:\\Personal Projects\\Video Surveilence System\\venv\\lib\\site-packages\\tensorflow\\__init__.py:41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[0;32m     40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[1;32m---> 41\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m KerasLazyLoader \u001b[39mas\u001b[39;00m _KerasLazyLoader\n\u001b[0;32m     43\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m _os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mTF2_BEHAVIOR\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'KerasLazyLoader' from 'tensorflow.python.util.lazy_loader' (e:\\Personal Projects\\Video Surveilence System\\venv\\lib\\site-packages\\tensorflow\\python\\util\\lazy_loader.py)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "# import tensorflow as tf\n",
    "# import argparse\n",
    "from PIL import Image\n",
    "import imutils\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab9182e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_loss(x1,x2):\n",
    "    difference=x1-x2\n",
    "    a,b,c,d,e=difference.shape\n",
    "    n_samples=a*b*c*d*e\n",
    "    sq_difference=difference**2\n",
    "    Sum=sq_difference.sum()\n",
    "    distance=np.sqrt(Sum)\n",
    "    mean_distance=distance/n_samples\n",
    "    return mean_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57672939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "model=tf.keras.models.load_model(\"./saved_model.h5\")\n",
    "cap = cv2.VideoCapture(\"./Avenue_Dataset/testing_videos/03.avi\")\n",
    "print(cap.isOpened())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "672d26ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1708/1497032685.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m700\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m600\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mframe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m227\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m227\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mINTER_AREA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mgray\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2989\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m0.5870\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m0.1140\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imutils\\convenience.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(image, width, height, inter)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;31m# grab the image size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;31m# if both the width and height are None, then return the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "while cap.isOpened():\n",
    "    imagedump=[]\n",
    "    ret,frame=cap.read()\n",
    "    for i in range(10):\n",
    "        ret,frame=cap.read()\n",
    "        image = imutils.resize(frame,width=700,height=600)\n",
    "        frame=cv2.resize(frame, (227,227), interpolation = cv2.INTER_AREA)\n",
    "        gray=0.2989*frame[:,:,0]+0.5870*frame[:,:,1]+0.1140*frame[:,:,2]\n",
    "        gray=(gray-gray.mean())/gray.std()\n",
    "        gray=np.clip(gray,0,1)\n",
    "        imagedump.append(gray)\n",
    "    imagedump=np.array(imagedump)\n",
    "    imagedump.resize(227,227,10)\n",
    "    imagedump=np.expand_dims(imagedump,axis=0)\n",
    "    imagedump=np.expand_dims(imagedump,axis=4)\n",
    "    output=model.predict(imagedump)\n",
    "    loss=mean_squared_loss(imagedump,output)\n",
    "    if frame.any()==None:\n",
    "        print(\"none\")\n",
    "    if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "        break\n",
    "    if loss>0.00068:\n",
    "        print('Abnormal Event Detected')\n",
    "        cv2.putText(image,\"Abnormal Event\",(100,80),cv2.FONT_HERSHEY_SIMPLEX,2,(0,0,255),4)\n",
    "    cv2.imshow(\"video\",image)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99eeb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "from keras.models import load_model\n",
    "import argparse\n",
    "from PIL import Image\n",
    "import imutils\n",
    "\n",
    "\n",
    "def mean_squared_loss(x1,x2):\n",
    "    difference=x1-x2\n",
    "    a,b,c,d,e=difference.shape\n",
    "    n_samples=a*b*c*d*e\n",
    "    sq_difference=difference**2\n",
    "    Sum=sq_difference.sum()\n",
    "    distance=np.sqrt(Sum)\n",
    "    mean_distance=distance/n_samples\n",
    "\n",
    "    return mean_distance\n",
    "\n",
    "\n",
    "model=load_model(\"C:\\\\Projects\\\\Video Surveilance\\\\Train path\\\\saved_model.h5\")\n",
    "\n",
    "cap = cv2.VideoCapture(\"C:\\\\Projects\\\\Video Surveilance\\\\Avenue_Dataset\\\\Avenue Dataset\\\\testing_videos\\\\03.avi\")\n",
    "print(cap.isOpened())\n",
    "\n",
    "while cap.isOpened():\n",
    "    imagedump=[]\n",
    "    ret,frame=cap.read()\n",
    "\n",
    "    for i in range(10):\n",
    "        ret,frame=cap.read()\n",
    "        image = imutils.resize(frame,width=700,height=600)\n",
    "\n",
    "        frame=cv2.resize(frame, (227,227), interpolation = cv2.INTER_AREA)\n",
    "        gray=0.2989*frame[:,:,0]+0.5870*frame[:,:,1]+0.1140*frame[:,:,2]\n",
    "        gray=(gray-gray.mean())/gray.std()\n",
    "        gray=np.clip(gray,0,1)\n",
    "        imagedump.append(gray)\n",
    "\n",
    "    imagedump=np.array(imagedump)\n",
    "\n",
    "    imagedump.resize(227,227,10)\n",
    "    imagedump=np.expand_dims(imagedump,axis=0)\n",
    "    imagedump=np.expand_dims(imagedump,axis=4)\n",
    "\n",
    "    output=model.predict(imagedump)\n",
    "\n",
    "    loss=mean_squared_loss(imagedump,output)\n",
    "\n",
    "    if frame.any()==None:\n",
    "        print(\"none\")\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "        break\n",
    "    if loss>0.00068:\n",
    "        print('Abnormal Event Detected')\n",
    "        cv2.putText(image,\"Abnormal Event\",(100,80),cv2.FONT_HERSHEY_SIMPLEX,2,(0,0,255),4)\n",
    "\n",
    "    cv2.imshow(\"video\",image)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7443e30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0.00021849622041102845\n",
      "0.00021924999777083183\n",
      "0.00022024953617262417\n",
      "0.0002183109716554327\n",
      "0.0002191211117955691\n",
      "0.00021972001287743332\n",
      "0.00021961474001450562\n",
      "0.00022113025393297753\n",
      "0.00021920022802751835\n",
      "0.00022015449153601246\n",
      "0.0002209990893762133\n",
      "0.00022215457337906828\n",
      "0.00022158591095609642\n",
      "0.0002204127025485939\n",
      "0.0002202239893751617\n",
      "0.00022210723688424972\n",
      "0.00022314079838648099\n",
      "0.00022277581323007752\n",
      "0.00022230758690404046\n",
      "0.00022295144893349716\n",
      "0.00022575357019489358\n",
      "0.0002270434058736443\n",
      "0.00022809388909687526\n",
      "0.00022956226607334586\n",
      "0.00023142182828336905\n",
      "0.00023355890788769474\n",
      "0.00023502629969890108\n",
      "0.0002337086401567939\n",
      "0.00023761510980276643\n",
      "0.00023706428339662512\n",
      "0.00023864983046418991\n",
      "0.00024217422051373163\n",
      "0.0002513705400480156\n",
      "Abnormal Event Detected\n",
      "0.00024891787021720887\n",
      "0.00025629840628754686\n",
      "Abnormal Event Detected\n",
      "0.0002619384687095425\n",
      "Abnormal Event Detected\n",
      "0.0002593721435120613\n",
      "Abnormal Event Detected\n",
      "0.0002570033302075653\n",
      "Abnormal Event Detected\n",
      "0.0002636079780004451\n",
      "Abnormal Event Detected\n",
      "0.00026474146682875684\n",
      "Abnormal Event Detected\n",
      "0.00026298374186886853\n",
      "Abnormal Event Detected\n",
      "0.0002815374110753302\n",
      "Abnormal Event Detected\n",
      "0.00028562705376318574\n",
      "Abnormal Event Detected\n",
      "0.0002760999689885809\n",
      "Abnormal Event Detected\n",
      "0.00027706581135115743\n",
      "Abnormal Event Detected\n",
      "0.00027064513402633264\n",
      "Abnormal Event Detected\n",
      "0.00027300090294043555\n",
      "Abnormal Event Detected\n",
      "0.00028969251481696746\n",
      "Abnormal Event Detected\n",
      "0.00027960651508053015\n",
      "Abnormal Event Detected\n",
      "0.0002677761188164419\n",
      "Abnormal Event Detected\n",
      "0.00026750610293161525\n",
      "Abnormal Event Detected\n",
      "0.0002675872020154965\n",
      "Abnormal Event Detected\n",
      "0.00027268944781366273\n",
      "Abnormal Event Detected\n",
      "0.00027649873278856195\n",
      "Abnormal Event Detected\n",
      "0.0002711859203823794\n",
      "Abnormal Event Detected\n",
      "0.000285357208266043\n",
      "Abnormal Event Detected\n",
      "0.0002645803158121814\n",
      "Abnormal Event Detected\n",
      "0.00026847481842025723\n",
      "Abnormal Event Detected\n",
      "0.00026627468587644753\n",
      "Abnormal Event Detected\n",
      "0.00026945204385570313\n",
      "Abnormal Event Detected\n",
      "0.0002687262275397227\n",
      "Abnormal Event Detected\n",
      "0.0002669722744783589\n",
      "Abnormal Event Detected\n",
      "0.0002608445135694577\n",
      "Abnormal Event Detected\n",
      "0.0002713459767191885\n",
      "Abnormal Event Detected\n",
      "0.00028809229023572265\n",
      "Abnormal Event Detected\n",
      "0.0002761772137230776\n",
      "Abnormal Event Detected\n",
      "0.0002802729384750357\n",
      "Abnormal Event Detected\n",
      "0.00031079591304304714\n",
      "Abnormal Event Detected\n",
      "0.00029391840121781467\n",
      "Abnormal Event Detected\n",
      "0.0003244184635914343\n",
      "Abnormal Event Detected\n",
      "0.0003123382465132263\n",
      "Abnormal Event Detected\n",
      "0.00024408353897874258\n",
      "0.00022184385696142353\n",
      "0.00022208758523619782\n",
      "0.00022197677119420084\n",
      "0.0002230449051094638\n",
      "0.000223324107394541\n",
      "0.00022326576895119972\n",
      "0.00022246009562932912\n",
      "0.00022325631784336547\n",
      "0.0002234772233905946\n",
      "0.00022252441488879204\n",
      "0.0002229236652891104\n",
      "0.0002228080511948772\n",
      "0.0002227125866910065\n",
      "0.00022262170393467907\n",
      "0.00022318567950484523\n",
      "0.00022332271114131782\n",
      "0.00022316372761081472\n",
      "0.0002232110549967847\n",
      "0.00022213173298817147\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3056/452035089.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m700\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m600\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mframe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m227\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m227\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mINTER_AREA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mgray\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2989\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m0.5870\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m0.1140\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imutils\\convenience.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(image, width, height, inter)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;31m# grab the image size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;31m# if both the width and height are None, then return the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "from keras.models import load_model\n",
    "import argparse\n",
    "from PIL import Image\n",
    "import imutils\n",
    "\n",
    "def mean_squared_loss(x1,x2):\n",
    "    difference=x1-x2\n",
    "    a,b,c,d,e=difference.shape\n",
    "    n_samples=a*b*c*d*e\n",
    "    sq_difference=difference**2\n",
    "    Sum=sq_difference.sum()\n",
    "    distance=np.sqrt(Sum)\n",
    "    mean_distance=distance/n_samples\n",
    "\n",
    "    return mean_distance\n",
    "\n",
    "model=load_model(\"./saved_model.h5\")\n",
    "\n",
    "cap = cv2.VideoCapture(\"./Avenue_Dataset/testing_videos/05.avii\")\n",
    "print(cap.isOpened())\n",
    "\n",
    "while cap.isOpened():\n",
    "    imagedump=[]\n",
    "    ret,frame=cap.read()\n",
    "\n",
    "    for i in range(10):\n",
    "        ret,frame=cap.read()\n",
    "        image = imutils.resize(frame,width=700,height=600)\n",
    "        frame=cv2.resize(frame, (227,227), interpolation = cv2.INTER_AREA)\n",
    "        gray=0.2989*frame[:,:,0]+0.5870*frame[:,:,1]+0.1140*frame[:,:,2]\n",
    "        gray=(gray-gray.mean())/gray.std()\n",
    "        gray=np.clip(gray,0,1)\n",
    "        imagedump.append(gray)\n",
    "\n",
    "    imagedump=np.array(imagedump)\n",
    "\n",
    "    imagedump.resize(227,227,10)\n",
    "    imagedump=np.expand_dims(imagedump,axis=0)\n",
    "    imagedump=np.expand_dims(imagedump,axis=4)\n",
    "\n",
    "    output=model.predict(imagedump)\n",
    "\n",
    "    loss=mean_squared_loss(imagedump,output)\n",
    "\n",
    "    if frame.any()==None:\n",
    "        print(\"none\")\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "        break\n",
    "        \n",
    "    print(loss)\n",
    "    if loss>=0.00023:\n",
    "        print('Abnormal Event Detected')\n",
    "        cv2.putText(image,\"Abnormal Event\",(100,80),cv2.FONT_HERSHEY_SIMPLEX,2,(0,0,255),4)\n",
    "\n",
    "    cv2.imshow(\"video\",image)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950ef2da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
